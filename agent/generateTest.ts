import fs from "fs"
import path from "path"
import { openai } from "./openaiClient"
import { collectRelatedFiles } from "./resolveDependencies"

/**
 * Constructs the prompt for the OpenAI model to generate a Vitest test file.
 * 
 * @param mainFilePath - Path of the source file.
 * @param mainContent - Content of the source file.
 * @param relatedContents - Array of contents from related dependencies or types.
 * @returns A formatted string containing the system instructions and source context.
 */
function buildPrompt(
    mainFilePath: string,
    mainContent: string,
    relatedContents: string[]
) {
    return `
You are a senior TypeScript engineer.

Generate a Vitest test file for the following source file.

Requirements:
- Use Vitest
- No snapshot tests
- Cover edge cases
- Validate thrown errors where applicable
- Do not mock unless necessary
- Follow clean, readable structure

SOURCE FILE:
${mainContent}

RELATED TYPES OR DEPENDENCIES:
${relatedContents.join("\n\n")}

Return ONLY the test file code.
`
}

/**
 * Coordinates the test generation process for a given file.
 * It collects dependencies, builds a prompt, and calls the OpenAI API.
 * 
 * @param filePath - Absolute path to the source file to generate tests for.
 * @returns The generated Vitest test code.
 */
export async function generateTestForFile(
    filePath: string
): Promise<string> {
    // Read the main source file content
    const mainContent = fs.readFileSync(filePath, "utf-8")

    // Gather content from related files (like types) to provide more context to the LLM
    const relatedPaths = collectRelatedFiles(filePath)
    const relatedContents = relatedPaths.map((p) =>
        fs.readFileSync(p, "utf-8")
    )

    // Create the instruction prompt
    const prompt = buildPrompt(
        filePath,
        mainContent,
        relatedContents
    )

    // Request a completion from OpenAI using the gpt-4o-mini model
    const response = await openai.chat.completions.create({
        model: "gpt-4o-mini",
        messages: [
            {
                role: "system",
                content:
                    "You are an expert TypeScript engineer specialized in writing high-quality tests.",
            },
            {
                role: "user",
                content: prompt,
            },
        ],
        temperature: 0.2, // Lower temperature for more deterministic/stable output
    })

    const content = response.choices[0].message.content

    if (!content) {
        throw new Error("No test generated by model")
    }

    return content
}